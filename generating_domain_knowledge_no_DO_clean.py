import sysimport multiprocessingimport torchfrom metro_data_convertor.Find_project_root import Find_project_rootfrom datetime import timedeltafrom metro_data_convertor.Generating_logit_probabilities import Generating_logit_probabilitiesfrom metro_data_convertor.Get_Time_DepartFreDic import Get_Time_DepartFreDicfrom metro_data_convertor.Process_Time_DepartFreDic import Process_Time_DepartFreDicfrom dmn_knw_gnrtr.fit_trip_generation_model import fit_trip_generation_modelfrom dmn_knw_gnrtr.generating_OD_section_pssblty_sparse_array_0209 import generating_OD_section_pssblty_sparse_array_0209import pickleimport numpy as npimport yamlimport osimport argparsefrom train_button import read_cfg_fileimport timeproject_root_main = os.path.dirname(os.path.abspath(__file__))  # 获取当前脚本的绝对路径if project_root_main not in sys.path:    sys.path.insert(0, project_root_main)def main():    time_start=time.time()    start_time_str = time.strftime("%Y%m%d_%H%M%S", time.localtime(time_start))    script_name = os.path.basename(__file__).split('.')[0]    file_name = f"{start_time_str}_{script_name}.txt"    def write_cfg_file(filename, cfg): #在指定路径下写入配置文件        with open(filename, 'w') as ymlfile:            yaml.dump(cfg, ymlfile, default_flow_style=False, allow_unicode=True) #将cfg写入ymlfile中    args = argparse.Namespace(config_filename=f'data{os.path.sep}config{os.path.sep}train_sz_dim26_units96_h4c512_250207.yaml') #设置参数，此处需要进行贝叶斯优化    cfg = read_cfg_file(args.config_filename) #读取配置文件    project_root = Find_project_root()    text_place_and_month = "suzhou_04"    base_dir = os.path.join(project_root, f"data{os.path.sep}{text_place_and_month}")    station_manager_dict_name='station_manager_dict_no_11.pkl'    graph_sz_conn_no_name='graph_sz_conn_no_11.pkl'    start_date_str = cfg['domain_knowledge']['start_date_str']    end_date_str = cfg['domain_knowledge']['end_date_str']    train_filename = os.path.join(base_dir, 'train_dict.pkl')    train_dict_file_path = os.path.join(base_dir, 'train_dict.pkl')    OD_path_visit_prob_dic_file_path = os.path.join(base_dir, 'OD_path_visit_prob_dic.pkl')    Time_DepartFreDic_file_path = os.path.join(base_dir, 'Time_DepartFreDic.pkl')    Time_DepartFreDic_Array_file_path = os.path.join(base_dir, 'Time_DepartFreDic_Array.pkl')    Time_DepartFreDic_filename = os.path.join(base_dir, 'Time_DepartFreDic.pkl')    suzhou_sub_data_file_path = os.path.join(base_dir, 'suzhou_sub_data.xlsx')    excel_path = os.path.join(base_dir, 'Suzhou_zhandian_no_11.xlsx')    graph_sz_conn_root = os.path.join(base_dir, f'{graph_sz_conn_no_name}')    station_index_root = os.path.join(base_dir, 'station_index_no_11.pkl')    station_manager_dict_root = os.path.join(base_dir, 'station_manager_dict_no_11.pkl')    result_API_root = os.path.join(base_dir, 'result_API_modified.xlsx')    time_interval = timedelta(minutes=int(cfg['domain_knowledge']['timedelta_minutes']))    OD_path_visit_prob_array_file_path = os.path.join(base_dir, 'OD_path_visit_prob_array.pkl')    station_manager_dict_file_path = f"{base_dir}{os.path.sep}{station_manager_dict_name}"    Date_and_time_OD_path_dic_file_path = os.path.join(base_dir, f'Date_and_time_OD_path_dic.pkl')    Date_and_time_OD_path_cp_factors_dic_file_path = os.path.join(base_dir, f'Date_and_time_OD_path_cp_factors_dic.pkl')    OD_feature_array_file_path = os.path.join(base_dir, f'OD_feature_array_dic.pkl')    OD_path_dic_file_path = os.path.join(base_dir, f'OD_path_dic.pkl')    #This is used to generate information for all paths corresponding to all OD pairs. If the road network remains unchanged, there is no need to run it.    """Generating_Metro_Related_data(excel_path, graph_sz_conn_root, station_index_root, station_manager_dict_root,result_API_root,                                      suzhou_sub_data_file_path, Time_DepartFreDic_filename, time_interval,                                  Time_DepartFreDic_file_path, Time_DepartFreDic_Array_file_path, OD_path_visit_prob_array_file_path,                                  train_dict_file_path, OD_path_visit_prob_dic_file_path, train_filename, start_date_str, end_date_str)"""    # Used to generate a logit distribution result under a specific parameter setting, format: (154, 154, 3, 1).    """Generating_logit_probabilities(train_dict_file_path, OD_path_visit_prob_dic_file_path,                                               station_manager_dict_file_path, graph_sz_conn_root, station_manager_dict_root, station_index_root, result_API_root)    """#Create a dictionary with timestamps as keys and subway departure frequency information as values.    """Get_Time_DepartFreDic(suzhou_sub_data_file_path, Time_DepartFreDic_filename, time_interval,                          excel_path, graph_sz_conn_root, station_index_root, start_date_str, end_date_str, station_manager_dict_root, result_API_root)    """    """generating_OD_section_pssblty_sparse_array_0209(base_dir, station_manager_dict_name,                                            Time_DepartFreDic_file_path, OD_path_dic_file_path,                                            station_manager_dict_file_path, OD_feature_array_file_path,                                            Date_and_time_OD_path_cp_factors_dic_file_path)"""    """for prefix in ("train","test","val"):        Time_DepartFreDic_Array_file_path = os.path.join(project_root, base_dir, f'{prefix}_Time_DepartFreDic_Array.pkl')        Process_Time_DepartFreDic(station_index_root, Time_DepartFreDic_file_path, Time_DepartFreDic_Array_file_path, prefix)"""    # Combine Time_DepartFreDic to obtain a time-vliuuarying OD_visiting_prob matrix, which is deprecated in the current version.    # Reprocessing_OD_visiting_prob(OD_path_visit_prob_dic_file_path, OD_path_visit_prob_array_file_path)    train_sql= cfg['domain_knowledge']['train_sql']    test_sql= cfg['domain_knowledge']['test_sql']    val_sql= cfg['domain_knowledge']['val_sql']    repeated_or_not_repeated="not_repeated"    host='localhost'    user='root'    password='zxczxc@1234'    database='suzhoudata0513'    #RGCN is trained independently.    Using_lat_lng_or_index = cfg['domain_knowledge']['Using_lat_lng_or_index']    if Using_lat_lng_or_index=="lat_lng": # 如果使用经纬度，则节点特征数为num_nodes+2，否则为num_nodes+1        RGCN_node_features = cfg['model']['num_nodes'] + 3    else:        RGCN_node_features = cfg['model']['num_nodes'] + 2    RGCN_hidden_units =  int(cfg['domain_knowledge']['RGCN_hidden_units'])    RGCN_output_dim = int(cfg['domain_knowledge']['RGCN_output_dim'])    RGCN_K = int(cfg['domain_knowledge']['RGCN_K'])    lr = int(cfg['domain_knowledge']['lr'])    epoch_num = int(cfg['domain_knowledge']['epoch_num'])    seq_len = cfg['model']['seq_len']    train_ratio=float(cfg['domain_knowledge']['train_ratio'])    input_dim_m_1 = cfg['model']['input_dim'] - 1    initial_gamma = float(cfg['domain_knowledge']['initial_gamma'])    lr_gener = float(cfg['domain_knowledge']['lr_gener'])    maxiter = int(cfg['domain_knowledge']['maxiter'])    from dmn_knw_gnrtr.generating_array_OD import generate_OD_DO_array    from dmn_knw_gnrtr.generating_array_OD import process_data    from dmn_knw_gnrtr.generating_array_OD import Connect_to_SQL    from dmn_knw_gnrtr.generating_repeated_or_not_repeated_domain_knowledge import generating_repeated_or_not_repeated_domain_knowledge    from dmn_knw_gnrtr.PYGT_signal_generation_one_hot import PYGT_signal_generation    from dmn_knw_gnrtr.run_PYGT_0917 import run_PYGT    from dmn_knw_gnrtr.test_PYGT_0917 import test_PYGT    for prefix in ("train","test","val"): #意思是对于训练集、测试集和验证集，分别进行以下操作，        Time_DepartFreDic_file_path = os.path.join(project_root, base_dir, 'Time_DepartFreDic.pkl')        station_manager_dict_file_path = os.path.join(base_dir, f"{station_manager_dict_name}")        train_result_array_OD_or_DO_file_path = os.path.join(base_dir, f'{prefix}_result_array.pkl')        normalization_params_file_path = os.path.join(f'data{os.path.sep}{text_place_and_month}', 'normalization_params.pkl')        if prefix!="train":            df,x_y_time,sz_conn_to_station_index,station_index_to_sz_conn=Connect_to_SQL(prefix, train_sql, test_sql, val_sql, station_manager_dict_name,                                                                                         host, user, password, database)            result_array_OD = generate_OD_DO_array(df, x_y_time, sz_conn_to_station_index, station_index_to_sz_conn, base_dir, prefix,station_manager_dict_name)        with open(station_manager_dict_file_path, 'rb') as f: #读取文件            station_manager_dict = pickle.load(f) #将文件中的数据加载到station_manager_dict中        result_array_file_path = os.path.join(base_dir, f'{prefix}_result_array.pkl')#结果路径        with open(result_array_file_path, 'rb') as f: #读取文件            intermediate_dic = pickle.load(f, errors='ignore')#将文件中的数据加载到intermediate_dic中            T_N_D_ODs_dic = intermediate_dic['T_N_D_ODs'] #获取T_N_D_ODs            O_data_dic = intermediate_dic['Trip_Production_In_Station_or_Out_Station']#获取O_data_dic            D_data_dic = intermediate_dic['Trip_Attraction_In_Station_or_Out_Station']#获取D_data_dic            sorted_times = sorted(T_N_D_ODs_dic.keys())#对T_N_D_ODs_dic中的key进行排序            T_N_D_OD_array = np.array([T_N_D_ODs_dic[time] for time in sorted_times]) #将T_N_D_ODs_dic中的数据按照key的顺序存入T_N_D_OD_array中            O_data = np.array([O_data_dic[time] for time in sorted_times])# 将O_data_dic中的数据按照key的顺序存入O_data中            D_data = np.array([D_data_dic[time] for time in sorted_times])# 将D_data_dic中的数据按照key的顺序存入D_data中            C_data = station_manager_dict['station_distance_matrix']#获取站点之间的距离矩阵            q_obs_data = T_N_D_OD_array #获取T_N_D_OD_array            time_steps = len(O_data)#获取O_data的长度            # The gravity model is trained independently.            optimal_gamma, a_fitted, b_fitted, q_predicted_list = fit_trip_generation_model(O_data, D_data, C_data, q_obs_data,                                                                                                 time_steps=time_steps, initial_gamma=initial_gamma, lr_gener=lr_gener, maxiter=maxiter)                sava_to_cfg=False            if sava_to_cfg:                config = read_cfg_file(args.config_filename)                config['trip_distribution']['gamma'] = optimal_gamma                config['trip_distribution']['a'] = a_fitted                config['trip_distribution']['b'] = b_fitted                write_cfg_file(args.config_filename, config)            pkl_filename = os.path.join(project_root, f"data{os.path.sep}{text_place_and_month}", 'trip_generation_trained_params.pkl') #将训练好的参数保存到pkl_filename中            trained_params = {                'gamma': optimal_gamma,                'a': a_fitted,                'b': b_fitted,            }            def save_to_pkl(data, filename):                with open(filename, 'wb') as file:                    pickle.dump(data, file)            #save_to_pkl(trained_params, pkl_filename)    for prefix in ("train", "test", "val"):  # 意思是对于训练集、测试集和验证集，分别进行以下操作，        for str_prdc_attr in ("prdc", "attr"): #对于prdc和attr，分别进行以下操作            PYGT_signal_generation(base_dir, prefix, station_manager_dict_name, graph_sz_conn_no_name,                                   station_manager_dict_file_path, graph_sz_conn_root,                                   train_result_array_OD_or_DO_file_path,                                   normalization_params_file_path, str_prdc_attr, Using_lat_lng_or_index)            if(prefix=="train"):                run_PYGT(base_dir, prefix, str_prdc_attr, RGCN_node_features, RGCN_hidden_units, RGCN_output_dim, RGCN_K, lr, epoch_num, train_ratio)                test_PYGT(base_dir, prefix, str_prdc_attr)            process_data(input_dim_m_1, base_dir, prefix, str_prdc_attr, seq_len)        generating_repeated_or_not_repeated_domain_knowledge(base_dir, prefix, repeated_or_not_repeated, seq_len)    # 计时记录    time_stop=time.time()    print(f"Time used: {time_stop-time_start} seconds.")#输出时间到本地文件    duration = time_stop - time_start    with open(file_name, 'w') as f:        f.write(f"开始时间: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time_start))}\n")        f.write(f"停止时间: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time_stop))}\n")        f.write(f"运行时长: {duration:.2f} 秒\n")    print(f"Time used: {time_stop-time_start} seconds.")if __name__ == "__main__":    multiprocessing.set_start_method('spawn', force=True)    main()